import torch
from transformers import LlamaForCausalLM, LlamaTokenizer, TrainingArguments, Trainer
from peft import LoraConfig, get_peft_model, TaskType, PeftModel
from datasets import load_dataset

# âœ… è®¾ç½® Llama-7B æ¨¡å‹è·¯å¾„
model_path = "/root/autodl-tmp/llama-7b"

# âœ… åŠ è½½ Tokenizer
tokenizer = LlamaTokenizer.from_pretrained(model_path, local_files_only=True)

# âœ… ä¿®å¤ `ValueError: Asking to pad but the tokenizer does not have a padding token`
tokenizer.pad_token = tokenizer.eos_token  # ä½¿ç”¨ `eos_token` ä½œä¸º `pad_token`

# âœ… åŠ è½½ Llama è¯­è¨€æ¨¡å‹
model = LlamaForCausalLM.from_pretrained(
    model_path,
    local_files_only=True,
    torch_dtype=torch.float16,
    device_map="auto"  # è‡ªåŠ¨åˆ†é…åˆ° GPU
)

print("âœ… Llama-7B æ¨¡å‹åŠ è½½æˆåŠŸï¼")

# âœ… é…ç½® LoRAï¼ˆä½ç§©é€‚é…ï¼‰
lora_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,  # è¯­è¨€æ¨¡å‹ä»»åŠ¡
    r=8,  # LoRA ä½ç§©ç»´åº¦ï¼ˆæ§åˆ¶å¯è®­ç»ƒå‚æ•°çš„å¤§å°ï¼‰
    lora_alpha=16,  # LoRA ç¼©æ”¾å› å­
    lora_dropout=0.05,  # Dropout é˜²æ­¢è¿‡æ‹Ÿåˆ
    target_modules=["q_proj", "v_proj"]  # ä»…è°ƒæ•´ Q å’Œ V æƒé‡ï¼Œå‡å°‘æ˜¾å­˜å ç”¨
)

# âœ… å°† Llama æ¨¡å‹è½¬æ¢ä¸º LoRA è®­ç»ƒæ¨¡å¼
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()  # æ˜¾ç¤ºå¯è®­ç»ƒçš„å‚æ•°é‡

# âœ… åŠ è½½æ•°æ®é›†ï¼ˆè¿™é‡Œä½¿ç”¨ Hugging Face æ•°æ®é›†ï¼‰
dataset = load_dataset("Abirate/english_quotes", split="train[:1000]")  # ä»…å– 1000 æ¡æ•°æ®

# âœ… Tokenize æ•°æ®é›†
def tokenize_function(examples):
    return tokenizer(examples["quote"], padding="max_length", truncation=True, max_length=256)

# âœ… ä¿®å¤ `dataset.map()` æŠ¥é”™ï¼Œä¿è¯ `num_proc` ä»…åœ¨ `datasets` æ”¯æŒæ—¶ä½¿ç”¨
try:
    tokenized_datasets = dataset.map(tokenize_function, batched=True, num_proc=4)
except:
    tokenized_datasets = dataset.map(tokenize_function, batched=True)

print("âœ… æ•°æ® Tokenization å®Œæˆï¼")

# âœ… è®­ç»ƒå‚æ•°
training_args = TrainingArguments(
    output_dir="./lora_llama7b",
    per_device_train_batch_size=1,  # LoRA å…è®¸å° batch è®­ç»ƒ
    per_device_eval_batch_size=1,
    num_train_epochs=1,  # è®­ç»ƒ 1 è½®ï¼ˆå¯è°ƒæ•´ï¼‰
    logging_dir="./logs",
    logging_steps=10,
    save_strategy="epoch",
    fp16=True,  # 16-bit è®­ç»ƒå‡å°‘æ˜¾å­˜
    gradient_accumulation_steps=8,  # é€‚ç”¨äºå° batch
    optim="adamw_torch",  # ä½¿ç”¨æ›´ç¨³å®šçš„ AdamW
    evaluation_strategy="steps",  # è¯„ä¼°ç­–ç•¥
    eval_steps=100,  # æ¯ 100 æ­¥è¯„ä¼°ä¸€æ¬¡
)

# âœ… ä½¿ç”¨ `Trainer` è¿›è¡Œ LoRA è®­ç»ƒ
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets,
)
trainer.train()

# âœ… ä¿å­˜å¾®è°ƒåçš„ LoRA æƒé‡
model.save_pretrained("./lora_llama7b")
tokenizer.save_pretrained("./lora_llama7b")

print("ğŸ‰ LoRA è®­ç»ƒå®Œæˆï¼Œå·²ä¿å­˜å¾®è°ƒåçš„æƒé‡ï¼")

# âœ… é‡Šæ”¾æ˜¾å­˜ï¼Œé˜²æ­¢ OOM
del model
torch.cuda.empty_cache()

# âœ… é‡æ–°åŠ è½½ Llama-7B å¹¶åº”ç”¨ LoRA è®­ç»ƒå¥½çš„å‚æ•°
base_model = LlamaForCausalLM.from_pretrained(
    model_path,
    local_files_only=True,
    torch_dtype=torch.float16,
    device_map="auto"
)

# âœ… åŠ è½½å¾®è°ƒåçš„ LoRA é€‚é…å™¨
model = PeftModel.from_pretrained(base_model, "./lora_llama7b")
model.to("cuda")  # ç¡®ä¿ LoRA æ¨¡å‹åœ¨ GPU ä¸Š

print("\nğŸ”„ ä½¿ç”¨ LoRA è®­ç»ƒåçš„æ¨¡å‹è¿›è¡Œæ¨ç†...\n")

input_text = "è¯·ç”¨ä¸­æ–‡ä»‹ç» Llama è¯­è¨€æ¨¡å‹ã€‚"

# âœ… è¿è¡Œ 10 æ¬¡æ¨ç†
for i in range(10):
    print(f"ğŸ”„ ç¬¬ {i+1} æ¬¡è°ƒç”¨ LLM...\n")

    # Tokenize è¾“å…¥æ–‡æœ¬
