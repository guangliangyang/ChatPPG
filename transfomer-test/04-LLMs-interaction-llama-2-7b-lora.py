import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# ã€1ã€‘è®¾ç½®æ¨¡å‹è·¯å¾„
# å¦‚æœæœ¬åœ°å·²ç»ä¸‹è½½æˆ–è½¬æ¢å¥½çš„ Llama 2-7B æ¨¡å‹æƒé‡ä½äº "/root/autodl-tmp/llama2-7b" æ–‡ä»¶å¤¹ä¸­
model_path = "/root/autodl-tmp/llama-7b/"

# ã€2ã€‘åŠ è½½ Tokenizer
tokenizer = AutoTokenizer.from_pretrained(
    model_path,
    local_files_only=True  # å¦‚æœä»æœ¬åœ°åŠ è½½ï¼Œè®¾ä¸º True
)

# ã€3ã€‘åŠ è½½ Llama 2-7B æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    local_files_only=True,
    torch_dtype=torch.float16,  # ä¸€èˆ¬ç”¨ float16ï¼›è‹¥æ˜¾å­˜ä¸è¶³å¯å°è¯• bfloat16 æˆ– 8bit
    device_map="auto",
    load_in_8bit=True  # 8bit é‡åŒ–ï¼Œè¿›ä¸€æ­¥é™ä½æ˜¾å­˜å ç”¨
)

print("âœ… Llama-2-7B åŠ è½½æˆåŠŸï¼\n")

# ã€4ã€‘ç¤ºä¾‹å¯¹è¯æˆ–é—®ç­”
user_input = "è¯·ä»‹ç»ä¸€ä¸‹ Llama 2-7B æ¨¡å‹çš„ç‰¹ç‚¹ã€‚"

# å°†è¾“å…¥è½¬ä¸ºå¼ é‡å¹¶ç§»åŠ¨åˆ° GPU
inputs = tokenizer(user_input, return_tensors="pt").to("cuda")

# ç”Ÿæˆæ–‡æœ¬
with torch.no_grad():
    output_ids = model.generate(
        inputs.input_ids,
        max_length=2048,   # é€‚å½“è°ƒå¤§è¾“å‡ºé•¿åº¦
        temperature=0.7,  # é‡‡æ ·æ¸©åº¦
        top_k=50,         # top-k é‡‡æ ·
        top_p=0.9,        # nucleus sampling
    )

# è§£ç ç”Ÿæˆç»“æœ
output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)

print("ğŸ“ Llama-2-7B ç”Ÿæˆçš„å›ç­”ï¼š")
print(output_text)
