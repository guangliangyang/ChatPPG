import torch
from transformers import LlamaForCausalLM, LlamaTokenizer, TrainingArguments, Trainer
from peft import LoraConfig, get_peft_model, TaskType, PeftModel
from datasets import load_dataset

# âœ… è®¾ç½® Llama-7B æ¨¡å‹è·¯å¾„
model_path = "/root/autodl-tmp/llama-7b"

# âœ… åŠ è½½ Tokenizer
tokenizer = LlamaTokenizer.from_pretrained(model_path, local_files_only=True)

# âœ… åŠ è½½ Llama è¯­è¨€æ¨¡å‹
model = LlamaForCausalLM.from_pretrained(
    model_path,
    local_files_only=True,
    torch_dtype=torch.float16,
    device_map="auto"  # è‡ªåŠ¨åˆ†é…åˆ° GPU
)

print("âœ… Llama-7B æ¨¡å‹åŠ è½½æˆåŠŸï¼")

# âœ… é…ç½® LoRA
lora_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,  # è¯­è¨€æ¨¡å‹ä»»åŠ¡
    r=8,  # LoRA ä½ç§©ç»´åº¦ï¼ˆæ§åˆ¶å¯è®­ç»ƒå‚æ•°çš„å¤§å°ï¼‰
    lora_alpha=16,  # LoRA ç¼©æ”¾å› å­
    lora_dropout=0.05,  # Dropout é˜²æ­¢è¿‡æ‹Ÿåˆ
    target_modules=["q_proj", "v_proj"]  # ä»…è°ƒæ•´ Q å’Œ V æƒé‡ï¼Œå‡å°‘æ˜¾å­˜å ç”¨
)

# âœ… å°† Llama æ¨¡å‹è½¬æ¢ä¸º LoRA æ¨¡å‹
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()  # æ˜¾ç¤ºå¯è®­ç»ƒçš„å‚æ•°é‡

# âœ… åŠ è½½æ•°æ®é›†ï¼ˆè¿™é‡Œä½¿ç”¨ Hugging Face æ•°æ®é›†ï¼‰
dataset = load_dataset("Abirate/english_quotes", split="train[:1000]")  # ä»…å– 1000 æ¡æ•°æ®
def tokenize_function(examples):
    return tokenizer(examples["quote"], padding="max_length", truncation=True)
tokenized_datasets = dataset.map(tokenize_function, batched=True)

# âœ… è®­ç»ƒå‚æ•°
training_args = TrainingArguments(
    output_dir="./lora_llama7b",
    per_device_train_batch_size=1,  # LoRA å…è®¸å° batch è®­ç»ƒ
    per_device_eval_batch_size=1,
    num_train_epochs=1,  # è®­ç»ƒ 1 è½®ï¼ˆå¯è°ƒæ•´ï¼‰
    logging_dir="./logs",
    logging_steps=10,
    save_strategy="epoch",
    fp16=True,  # 16-bit è®­ç»ƒå‡å°‘æ˜¾å­˜
    gradient_accumulation_steps=4,  # é€‚ç”¨äºå° batch
)

# âœ… ä½¿ç”¨ Hugging Face `Trainer` è®­ç»ƒ
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets,
)
trainer.train()

# âœ… ä¿å­˜å¾®è°ƒåçš„ LoRA æƒé‡
model.save_pretrained("./lora_llama7b")

print("ğŸ‰ LoRA è®­ç»ƒå®Œæˆï¼Œå·²ä¿å­˜å¾®è°ƒåçš„æƒé‡ï¼")

# âœ… ä½¿ç”¨è®­ç»ƒå¥½çš„ LoRA æ¨¡å‹è¿›è¡Œæ¨ç†
print("\nğŸ”„ ä½¿ç”¨ LoRA è®­ç»ƒåçš„æ¨¡å‹è¿›è¡Œæ¨ç†...\n")

# é‡æ–°åŠ è½½ LoRA è®­ç»ƒå¥½çš„æ¨¡å‹
model = PeftModel.from_pretrained(model, "./lora_llama7b")

input_text = "è¯·ç”¨ä¸­æ–‡ä»‹ç» Llama è¯­è¨€æ¨¡å‹ã€‚"

# è¿è¡Œ 10 æ¬¡æ¨ç†
for i in range(10):
    print(f"ğŸ”„ ç¬¬ {i+1} æ¬¡è°ƒç”¨ LLM...\n")

    # Tokenize è¾“å…¥æ–‡æœ¬
    inputs = tokenizer(input_text, return_tensors="pt").to("cuda")

    # æ‰§è¡Œæ¨ç†ï¼ˆç”Ÿæˆæ–‡æœ¬ï¼‰
    with torch.no_grad():
        output_ids = model.generate(
            inputs.input_ids,
            max_length=200,  # ç”Ÿæˆæœ€å¤§é•¿åº¦
            temperature=0.7,  # é‡‡æ ·æ¸©åº¦
            top_k=50,  # é™åˆ¶ top-k é‡‡æ ·
            top_p=0.9,  # nucleus sampling
        )

    # è§£ç ç”Ÿæˆçš„æ–‡æœ¬
    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)

    # æ‰“å°ç»“æœ
    print(f"ğŸ“ ç¬¬ {i+1} æ¬¡ LLM ç”Ÿæˆçš„æ–‡æœ¬ï¼š")
    print(output_text)
    print("=" * 80)

print("ğŸ‰ æ‰€æœ‰ 10 æ¬¡æ¨ç†å®Œæˆï¼")
