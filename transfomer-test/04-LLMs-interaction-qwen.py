from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
import time

# âœ… Set model path for Qwen
model_path = "/root/autodl-tmp/DeepSeek-R1-Distill-Qwen-1.5B"

# âœ… Load Qwen Tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)

# âœ… Load Qwen Model
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    trust_remote_code=True,
    torch_dtype=torch.float16,  # Use float16 to reduce memory usage
    device_map="auto"  # Auto-assign GPU
)

print("âœ… Qwen model loaded successfully!\n")

# âœ… Input text
input_text = "å‘Šè¯‰æˆ‘å‡ ä¸ªå¯ä»¥æŠ•èµ„çš„è‚¡ç¥¨ï¼Œä»€ä¹ˆæ—¶å€™ä¹°å–ã€‚"

# âœ… Run 10 inference loops and measure execution time
total_time = 0.0
num_iterations = 10

for i in range(num_iterations):
    print(f"ğŸ”„ Running LLM inference {i + 1}...\n")

    # Tokenize the input text
    inputs = tokenizer(input_text, return_tensors="pt").to("cuda")

    # Measure inference time
    start_time = time.time()

    # Perform inference (text generation)
    with torch.no_grad():
        output_ids = model.generate(
            inputs.input_ids,
            max_length=200,  # Set max generation length
            temperature=0.7,  # Sampling temperature
            top_k=50,  # Top-k sampling
            top_p=0.9,  # Nucleus sampling
        )

    end_time = time.time()
    inference_time = end_time - start_time
    total_time += inference_time

    # Decode generated text
    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)

    # Print the generated output
    print(f"ğŸ“ LLM Output {i + 1}:")
    print(output_text)
    print(f"â±ï¸ Inference Time: {inference_time:.4f} seconds")
    print("=" * 80)

# Calculate and print the average inference time
average_time = total_time / num_iterations
print(f"ğŸ¯ Average LLM Inference Time: {average_time:.4f} seconds")
print("ğŸ‰ All 10 inferences completed!")
